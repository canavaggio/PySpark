{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this notebook is to predict whether a customer would be interested in Vehicle Insurance \n",
    "\n",
    "##### The data is as follows:\n",
    "\n",
    "* `id` — Unique ID for the customer\n",
    "* `Gender` — Gender of the customer\n",
    "* `Age` —  Age of the customer\n",
    "* `Driving_License` — 0 : Customer does not have DL, 1 : Customer already has DL\n",
    "* `Region_Code` — Unique code for the region of the customer\n",
    "* `Previously_Insured` — 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
    "* `Vehicle_Age` — Age of the Vehicle\n",
    "* `Vehicle_Damage` — 1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
    "* `Annual_Premium` — The amount customer needs to pay as premium in the year\n",
    "* `PolicySalesChannel` — Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
    "* `Vintage` — Number of Days, Customer has been associated with the company\n",
    "* `Response` — 1 : Customer is interested, 0 : Customer is not interested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up  SparkSession\n",
    "##  A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, \n",
    "## cache tables, and read parquet files. \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Distributed big data project / PySpark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up SparkSession  [doc](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Distributed big data project / PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0bc08ee9a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "|  1|  Male| 44|              1|       28.0|                 0|  > 2 Years|           Yes|       40454.0|                26.0|    217|       1|\n",
      "|  2|  Male| 76|              1|        3.0|                 0|   1-2 Year|            No|       33536.0|                26.0|    183|       0|\n",
      "|  3|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|                26.0|     27|       1|\n",
      "|  4|  Male| 21|              1|       11.0|                 1|   < 1 Year|            No|       28619.0|               152.0|    203|       0|\n",
      "|  5|Female| 29|              1|       41.0|                 1|   < 1 Year|            No|       27496.0|               152.0|     39|       0|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Driving_License: integer (nullable = true)\n",
      " |-- Region_Code: double (nullable = true)\n",
      " |-- Previously_Insured: integer (nullable = true)\n",
      " |-- Vehicle_Age: string (nullable = true)\n",
      " |-- Vehicle_Damage: string (nullable = true)\n",
      " |-- Annual_Premium: double (nullable = true)\n",
      " |-- Policy_Sales_Channel: double (nullable = true)\n",
      " |-- Vintage: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## loading the data \n",
    "df = spark.read.load(\"/home/charles/M1/distribured-bigdata-system/project/data/train.csv\",\n",
    "                     format=\"csv\",\n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\",\n",
    "                     header=\"true\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any') # first let's drop the NA value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Driving_License: integer (nullable = true)\n",
      " |-- Region_Code: double (nullable = true)\n",
      " |-- Previously_Insured: integer (nullable = true)\n",
      " |-- Vehicle_Age: string (nullable = true)\n",
      " |-- Vehicle_Damage: string (nullable = true)\n",
      " |-- Annual_Premium: double (nullable = true)\n",
      " |-- Vintage: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The column id is not revelant  is just the identifer user. Let's drop it. \n",
    "\n",
    "df = df.select('Gender',\n",
    "               'Age',\n",
    "               'Driving_License',\n",
    "               'Region_Code',\n",
    "               'Previously_Insured',\n",
    "               'Vehicle_Age',\n",
    "               'Vehicle_Damage',\n",
    "               'Annual_Premium',\n",
    "               'Vintage',\n",
    "               'Response')\n",
    "\n",
    "cols = df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform descriptive analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               Age|    Annual_Premium|           Vintage|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|            381109|            381109|            381109|\n",
      "|   mean|38.822583565331705|30564.389581458323|154.34739667654136|\n",
      "| stddev|15.511611018095321|17213.155056980126|  83.6713036265871|\n",
      "|    min|                20|            2630.0|                10|\n",
      "|    max|                85|          540165.0|               299|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Age',\n",
    "          'Annual_Premium',\n",
    "          'Vintage').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+------------------+------------------+-----------+--------------+\n",
      "|summary|Gender|    Driving_License|       Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|\n",
      "+-------+------+-------------------+------------------+------------------+-----------+--------------+\n",
      "|  count|381109|             381109|            381109|            381109|     381109|        381109|\n",
      "|   mean|  null| 0.9978693759528114|26.388807401557035|0.4582101183650871|       null|          null|\n",
      "| stddev|  null|0.04610954420779957|13.229888025788474| 0.498251198887226|       null|          null|\n",
      "|    min|Female|                  0|               0.0|                 0|   1-2 Year|            No|\n",
      "|    max|  Male|                  1|              52.0|                 1|  > 2 Years|           Yes|\n",
      "+-------+------+-------------------+------------------+------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Gender',\n",
    "          'Driving_License',\n",
    "          'Region_Code',\n",
    "          'Previously_Insured',\n",
    "          'Vehicle_Age',\n",
    "          'Vehicle_Damage'\n",
    "         ).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting, transforming and selecting features [doc](https://spark.apache.org/docs/latest/ml-features.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# categoricalColumns contain all the feature encode as string\n",
    "categoricalColumns = ['Gender',\n",
    "                      'Driving_License',\n",
    "                      'Region_Code',\n",
    "                      'Previously_Insured',\n",
    "                      'Vehicle_Age',\n",
    "                      'Vehicle_Damage',\n",
    "                     ]\n",
    "\n",
    "stages = [] # empty list\n",
    "\n",
    "for categoricalCol in categoricalColumns :\n",
    "    \n",
    "    # StringIndexer encodes a string column of labels to a column of label indices\n",
    "    # If the input column is numeric, StringIndexer cast it to string and index the string values\n",
    "    stringIndexer = StringIndexer(\n",
    "        inputCol = categoricalCol, \n",
    "        outputCol = categoricalCol + '_Index'\n",
    "    ) # indexer\n",
    "    \n",
    "    # One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a \n",
    "    # single one-value indicating the presence of a specific feature value from among the set of all feature values.\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=[stringIndexer.getOutputCol()],\n",
    "        outputCols=[categoricalCol + \"_classVec\"]\n",
    "    )\n",
    "    \n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'Response', outputCol = 'label') \n",
    "\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols = ['Age','Annual_Premium','Vintage']\n",
    "\n",
    "assemblerInputs = [c + \"_classVec\" for c in categoricalColumns] + numericCols\n",
    "\n",
    "# VectorAssembler is a transformer that combines a given list of columns into a single vector column.\n",
    "# VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type.\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'features')\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline [doc](https://spark.apache.org/docs/latest/ml-pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Driving_License: integer (nullable = true)\n",
      " |-- Region_Code: double (nullable = true)\n",
      " |-- Previously_Insured: integer (nullable = true)\n",
      " |-- Vehicle_Age: string (nullable = true)\n",
      " |-- Vehicle_Damage: string (nullable = true)\n",
      " |-- Annual_Premium: double (nullable = true)\n",
      " |-- Vintage: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "df = pipelineModel.transform(df)\n",
    "\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+---+---------------+-----------+------------------+-----------+--------------+--------------+-------+--------+\n",
      "|label|            features|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Vintage|Response|\n",
      "+-----+--------------------+------+---+---------------+-----------+------------------+-----------+--------------+--------------+-------+--------+\n",
      "|  1.0|(61,[0,1,2,54,57,...|  Male| 44|              1|       28.0|                 0|  > 2 Years|           Yes|       40454.0|    217|       1|\n",
      "|  0.0|(61,[0,1,10,54,55...|  Male| 76|              1|        3.0|                 0|   1-2 Year|            No|       33536.0|    183|       0|\n",
      "|  1.0|(61,[0,1,2,54,57,...|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|     27|       1|\n",
      "|  0.0|(61,[0,1,11,56,58...|  Male| 21|              1|       11.0|                 1|   < 1 Year|            No|       28619.0|    203|       0|\n",
      "|  0.0|(61,[1,5,56,58,59...|Female| 29|              1|       41.0|                 1|   < 1 Year|            No|       27496.0|     39|       0|\n",
      "+-----+--------------------+------+---+---------------+-----------+------------------+-----------+--------------+--------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier  [doc](https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+----------------+----------+--------------------+\n",
      "|Age|Previously_Insured|label|   rawPrediction|prediction|         probability|\n",
      "+---+------------------+-----+----------------+----------+--------------------+\n",
      "| 20|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 21|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 21|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 21|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 21|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 21|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 21|                 0|  0.0|   [474.0,102.0]|       0.0|[0.82291666666666...|\n",
      "| 22|                 0|  0.0|[16904.0,2816.0]|       0.0|[0.85720081135902...|\n",
      "| 22|                 0|  0.0|[16904.0,2816.0]|       0.0|[0.85720081135902...|\n",
      "| 22|                 0|  0.0|[16904.0,2816.0]|       0.0|[0.85720081135902...|\n",
      "+---+------------------+-----+----------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 6)\n",
    "\n",
    "dtModel = dt.fit(train)\n",
    "\n",
    "predictions = dtModel.transform(test)\n",
    "\n",
    "predictions.select('Age', 'Previously_Insured', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8258442149484794\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation using Cross-Validation [doc](https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8288756236113918\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.impurity, ['gini','entropy'])\n",
    "             .addGrid(dt.maxDepth, [5, 6, 10])\n",
    "             .addGrid(dt.minInfoGain, [0.0, 0.2, 0.3])\n",
    "             .addGrid(dt.minInstancesPerNode, [1, 2])\n",
    "            ).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=dt,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvmodel = cv.fit(train)\n",
    "\n",
    "# Make predictions on test data. cvModel uses the best model found.\n",
    "pred = cvmodel.transform(test)\n",
    "\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(pred, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier [doc](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+--------------------+----------+--------------------+\n",
      "|Age|Previously_Insured|label|       rawPrediction|prediction|         probability|\n",
      "+---+------------------+-----+--------------------+----------+--------------------+\n",
      "| 20|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 21|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 21|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 21|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 21|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 21|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 21|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 22|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 22|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 22|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 22|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 22|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 22|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 22|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 23|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 24|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 24|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 25|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 25|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 25|                 0|  0.0|[86.9329115242932...|       0.0|[0.86932911524293...|\n",
      "| 25|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 25|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 26|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 27|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 27|                 0|  0.0|[87.1189224578159...|       0.0|[0.87118922457815...|\n",
      "| 27|                 0|  0.0|[87.1840492275651...|       0.0|[0.87184049227565...|\n",
      "| 27|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 27|                 0|  0.0|[86.8677847545440...|       0.0|[0.86867784754544...|\n",
      "| 28|                 0|  0.0|[86.7099268097512...|       0.0|[0.86709926809751...|\n",
      "| 28|                 0|  0.0|[86.4587891064792...|       0.0|[0.86458789106479...|\n",
      "+---+------------------+-----+--------------------+----------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "rf_predictions = rf_model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "rf_predictions.select('Age', 'Previously_Insured', 'label', 'rawPrediction', 'prediction', 'probability').show(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8029106459018723\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(rf_predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation using cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8488420995052295\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.impurity, ['gini','entropy'])\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "             .addGrid(rf.numTrees, [20, 100, 200])\n",
    "            ).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=rf_evaluator, numFolds=5)\n",
    "cvmodel = cv.fit(train)\n",
    "pred = cvmodel.transform(test)\n",
    "\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(pred, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
